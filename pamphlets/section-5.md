## 0056_1_Introduction_Demo_Overview
### introduction and demo overview:
### Why kubernetes?
The benefits of microservices come with complexity and one of those complexities is **running** the microservices. We just saw deploying the microservice app with
docker compose. But in real projects, running and managing large microservices apps with just a simple docker compose, can be hard especially if you want to
scale your services, like run multiple instances of each microservice or if you end up having 100s of microservices for your app.

You will start having challenges like not knowing when a microservice is down, having to manually restart services and their instances when they fail or getting
unexpected results when one microservice is down or unhealthy and ... .

So managing running microservices may become challenging without a proper mechanism in place.

![img.png](../img/section-5/0056_1-1.png)

For example netflix has 100s of microservices and 100s of 1000s of **instances** for these services. If any of those instances is down, how would they know?
They would need to have some kind of a system to monitor all of these microservices and the instances running in a deployment env.

![img.png](../img/section-5/0056_1-2.png)

That's where container orchestration tools come in to handle challenges of running microservice apps. With these tools, you can:
- flexibly scale up and down the instances. For example if the app is under heavy load or heavily used during the weekend, you can schedule 100s more instances
during weekend and remove them or kill them on specific days and all of these will be easy with these tools
- automate scaling up and down during certain times
- even if a microservice is down or it fails because of an error , the orchestration engine can automatically restart it. Again, you don't have to detect which
instances are down and restart them, it all happens automatically
- other benefits(look at the slide below for benefits of these tools when running microservices apps)
![img.png](../img/section-5/0056_1-3.png)

![img.png](../img/section-5/0056_1-4.png)

We're gonna use k8s as a deployment environment for our microservice app.

We will focus on how to deploy from gitlab CI/CD pipeline to an already running k8s cluster.

### Demo overview:
We need a dedicated user in k8s for gitlab, so gitlab can connect to k8s and deploy apps

![img.png](../img/section-5/0056_1-5.png)

## 0057_2_Create_a_K8s_cluster_on_LKE
### Create k8s cluster:
First we need to create a managed k8s cluster on linode platform.
![img.png](../img/section-5/0057_2-1.png)
![img.png](../img/section-5/0057_2-2.png)

### Create k8s cluster on LKE:
On LKE k8s dashboard, 2 of the smallest node would be enough for our demo.

You will get a k8s api endpoint and a downloader kubeconfig.yaml file.

![img.png](../img/section-5/0057_2-3.png)

### Connect to the k8s cluster:
Note: kubectl == kubernetes cli to interact with a cluster

When we create a k8s cluster, usually a default admin user is created automatically for the cluster. So that when we run:
kubectl commands, we can access the cluster and administer it, using that admin user, but how do we tell k8s or kubectl that we're using the
k8s admin user. If we do `kubectl ...`, obviously it doesn't know which cluster you want to connect to, which user to use and ... .


![img.png](../img/section-5/0057_2-4.png)

For the admin user, an access file(also called kubeconfig file) also gets generated which is the file that has all the info about the cluster location or
cluster endpoint of the api server and all the credentials of admin user and ... and we need to pass that file to kubectl command, so that it knows how to connect
to the cluster.

So download that kubeconfig file. In that file, the cluster certificate which is at clusters>cluster>certificate-authority-data and it ensures that the access file(kubeconfig file) was
generated and signed by our cluster certificate and not generated by some random source, so you can't easily manipulate it.

In k8s we have service accounts which represent usually a non-human user and a service account has an access token and it's in kubeconfig file and in it's users>user>token, so it can
authenticate itself with the cluster or with the api server, when it connects to it and a service account also has permissions. Obviously the admin user has admin
priviliges, so it can administer the cluster, meaning he can access all namespaces, CRUD in those namespaces and ... .

![img.png](../img/section-5/0057_2-5.png)
![img.png](../img/section-5/0057_2-6.png)

We're gonna use kubeconfig file to access the cluster and administer it, however since it has some sensitive info, we need to set limited file access permissons to it first before using it.
So run(we did the same with ec2 instance's private ssh key to ssh to it):
```shell
chmod 400 <path to kubeconfig file>
```

We need to pass this file to kubectl commands to tell it to go do those things in k8s cluster using that info in kubeconfig file and one of the ways to pass that file to kubectl command is by
exporting a variable called KUBECONFIG and as it's value, use the path to download kubeconfig file:

Now kubectl will read that variable and use it to access the cluster and run commands on it.
Now if you run:
```shell
kubectl cluster-info
```
we should be able to conenct to the right cluster and get info from it.
![img.png](../img/section-5/0057_2-7.png)

So this env variable tells kubectl where to find the kubeconfig file or if you have multiple kubeconfig files, which one to use.

Now we can do anything that an admin user that is defined in that kubeconfig file, is allowed to do.

![img.png](../img/section-5/0057_2-8.png)

## 0058_3_Create_GitLab_User_with_restricted_Permissions
### Create gitlab user in k8s:
### Why create own user?
Kubectl is k8s cli that you can install on every machine. If we take the admin user's kubeconfig file and give it to gitlab runner, we would be able to
execute commands and deploy apps from gitlab runner to the k8s cluster, as long as it has that kubeconfig file. But do we want to give gitlab an admin user's credentials?
What does gitlab need to do in the cluster?
It needs to create a deployemnt and service resources in a specific namespace. Maybe also create configMap and secret for the application that it's deploying. But that's it.
It surely doesn't need to administer anything in the cluster or have access to the system namespace like the kubesystem and the security best practice is: you should
always give each user, only the privileges it needs and not more. Because if someone hacked into your gitlab, they would have full access of your cluster and that would
be able to do a lot of damage with it.
![img.png](../img/section-5/0058_3-1.png)

So we're gonna do this according to k8s security best practices for CI/CD pipelines and create a dedicated service account for gitlab and give that service account only those
permissions that it needs to do it's job. We will then generate the kubeconfig file for that service account and give gitlab that kubeconfig file to execute kubectl commands with it.

![img.png](../img/section-5/0058_3-2.png)

So k8s adminsitrator will have it's admin user and gitlab will have it's gitlab user.

![img.png](../img/section-5/0058_3-3.png)

### Create namespace:
In our case, we're gonna deploy our apps in a dedicated namespace called for example my-micro-service and we're gonna give gitlab permissions to that namespace.

![img.png](../img/section-5/0058_3-4.png)

First create the namespace itself:
```shell
kubectl create namespace my-micro-service
kubectl get namespace # for validation
```

### Create dedicated user & permission:
Let's create a service account for gitlab:
```shell
kubectl create serviceaccount cicd-sa --namespace=my-micro-service
```

Now this service account needs some permission to do stuff inside the namespace that it was created for, in order to deploy our microservice apps:

In k8s, permissions are given using resources called `roles`. Role = contains rules that represent a set of permissions.

Let's create that role and we create that using a k8s yaml file, let's call it cicd-role.yaml .

```shell
vim ci-cdrole.yaml
```
The file content:
```yaml
apiVersion: rbac.authorization.k8s.io/vl
kind: Role # The resource here is Role.
metadata:
  namespace: my-micro-service
  name: cicd-role # the name of the role
rules:
  - apiGroups: [""] # indicates the core API group 
    resources: ["pods", "services"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["extensions", "apps"] 
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```
Here, we're creating a Role for my-micro-service namespace.

Note: A role always sets permissions within a particular namespace.

Now run this to apply that file:
```yaml
kubectl apply -f <file name>
```

Now that we have created both service account and role, we have to associate them together, so we know that this service account has this role and for this, we have
another resource called `role-binding`. Because you can have 1 role and you can use it for multiple users or service accounts like `developer-role` and you can give it to
all the developers in the team.

![img.png](../img/section-5/0058_3-5.png)

To create a role-binding(in our case, we called it `cicd-rb` (rb stands for role-binding)) and we're gonna pass this command 3 options:
1) The role that we're binding to
2) service account that we're binding to(<namespace>:<service account name>)
3) namespace

```shell
kubectl create rolebinding cicd-rb --role=cicd-role --serviceaccount=my-micro-service:cicd-sa --namespace=my-micro-service
```

Now we have role binding which means our service account has a role associated to it now. So we're gonna be able to do some stuff when we use that service account.

### Create kubeconfig file:
Now that we have the user or service-account for gitlab, we're gonna create the kubeconfig access file for that service account. For that, we can simply just copy the
admin kubeconfig file and replace all the relevant values with gitlab's user and serviceaccount token:
```shell
cp <path to admin's kubeconfig file'> <path to new kubeconfig file for cicd user like: ~/cicd-kubeconfig.yaml>
vim <path to cicd user kubeconfig file>
```
In that new file, you need to replace the users>token. But how do we get that token of cicd service account user? With this:
```shell
kubectl get serviceaccount -n my-micro-service
```

![img.png](../img/section-5/0058_3-6.png)

Whenever we create a service account in a cluster, an associated secret resource gets created and this secret holds the token. So the service account itself doesn't have
the access token itself because it's a secret, so it's saved separately in a secret component and it's just referenced inside that service account and to see that
reference to a secret for the access token of a service account named cicd-sa, run:
```shell
kubectl get serviceaccount cicd-sa -n my-micro-service -o yaml
```
which would result in:

![img.png](../img/section-5/0058_3-7.png)
So it doesn't have the token directly inside it, it just references the secret that has a token.

Now grab the secret name and run this command to get the actual value of access token stored in that secret:
```shell
kubectl get secret <name of the secret, like: cicd-sa-token-4fkgn> cicd-sa -n my-micro-service -o yaml
```
now you can see the access token of cicd-sa user.

Now to decode that token, run:
```shell
echo <token value(long text!)> | base64 -D
```
Then copy the value(don't include the last percentage sign in the decoded text) and in kubeconfig file of gitlab cicd user, change the `token` value to that copied one
and also change the user>name key from: `...-admin` to `...-cicd` (it doesn't have to be the service-account name).

![img.png](../img/section-5/0058_3-8.png)

So update the contexts>context>user to the same name you chose earlier.

So now we have configured the kubeconfig file for our a cicd service account.

To test that the user's access work in the cluster(has the right permissions and not have permissions to the things we didn't define), we can overwrite the
`KUBECONFIG` env variable which currently points to the kubeconfig file of admin user and instead, temporarily, reference the kubeconfig file of cicd user:
```shell
export KUBECONFIG=<path to the cicd user's kubeconfig file'>
```
Then for example run:
```shell
kubectl get namespace # Forbidden

# or getting pods from a namespace that it doesn't have access to, should result in Forbidden:
kubectl get pod -n kube-system # Forbidden
```
it should not be able to do that(should give `Forbidden`), because it doesn't have permission to do it, but for example if that user tries to get pods in the namespace that
it has access to it, it should be able to do it, in our case, we **can** run:
```shell
kubectl get pod -n my-micro-service

# or:
kubectl get service -n my-micro-service
```
Now we have a cicd user configuration inside our cluster with right permissions.

Now revert that KUBECONFIG env var to point to the admin's kubeconfig file.

### Add kubeconfig file in gitlab:
Now take that kubeconfig file of cicd's user for k8s and make it available for our pipelines in gitlab, to be able to execute kubectl commands in those pipelines and it's a file
that all the services of our microservice will need, so we're gonna create a **shared variable** which is in gitlab's ui and in settings>CI/CD>variables section.
There, create a new variable called KUBE_CONFIG as a file type variable and paste the contents of the kubeconfig yaml file of that cicd user of k8s(make sure
the indentation of pasted text is correct).

Now we have the kubeconfig file available for all of our pipelines.

![img.png](../img/section-5/0058_3-9.png)
## 0059_4_Deploy_to_Kubernetes_Part_1
### Deploy to k8s - part 1:
All the things we need to do for deploying services to k8s:

k8s manifests files for deployment and service components for each microservice. We will also need a docker registry secret inside k8s, so that k8s can fetch the images inside the
cluster. Because they're in a private registry, so we need explicit access to it. We will also need kubectl tool installed on the gitlab runner, to actually deploy
k8s manifests and finally we will need to change deploy job from docker-compose(which is what we're currently using) to k8s deployment.
![img.png](../img/section-5/0059_4-1.png)
![img.png](../img/section-5/0059_4-2.png)

### Create deployment & service for frontend:
In frontend service, create deployment and service config files for k8s. Create them in a folder named kubernetes and create 2 files for service and deployment(you can
have both in 1 file).

We need to make the containers>image attr in deployment.yaml , configurable, because we're building the image inside the pipeline and we always increment the version,
so at least the image version should be dynamically set(we can't hard code it in that yaml file). But in fact, we're gonna make the whole configuration generic, to be able to reuse it.
So use variables in those deployment.yaml files, so that we would have a generic config and we can copy and paste it for all the services.


![img.png](../img/section-5/0059_4-3.png)
![img.png](../img/section-5/0059_4-4.png)
![img.png](../img/section-5/0059_4-5.png)

Note: We're gonna copy and paste k8 manifest files for other services as well, if we have 50 services, this will be not efficient to have 50 copies of the same
files for all your services.

![img.png](../img/section-5/0059_4-6.png)
![img.png](../img/section-5/0059_4-7.png)

If you have k8s manifests files that are the same across multiple services or apps, it's a common practice to have a separate git repo for the k8s manifest files which is
parameterized, generic and can be reused for multiple projects by setting it's variables. You would have a separate project for it, maybe in a group and you can
reference it in any project you need.

![img.png](../img/section-5/0059_4-8.png)

But in our case, we're gonna keep it simple and we're gonna copy them in each repo and in each service.

### Fix microservices endpoints:
Regarding communication of frontend and our services, in our previous deployments with docker-compose, we used the Dockerfile that set the endpoints(like PRODUCTS_SERVICE env
variable) of products service and shopping-cart service to **container names** that docker-compose was producing for us. So for example in PRODUCTS_SERVICE="products_app_1", 
the `products_app_1` is docker-compose specific. 

![img.png](../img/section-5/0059_4-9.png)

Now since we're creating k8s services for our microservice deployments, we would need to change these values to point to
**k8s service names** instead. So in k8s service yaml file for each microservice, we're calling the service whatever the microservice name is. So frontend will have a service
called frontend, products will have a k8s service called products and ... . These are gonna be the endpoint name inside k8s for those services, so we have to change those as well.

So instead of: `PRODUCTS_SERVICE=products_app_1` , the endpoint name is gonna be the k8s service name, so: `PRODUCTS_SERVICE=products` why ? Because k8s services are components which
let different applications inside, communicate with each other and they can talk to each other using those service names like `products` in our case.

So add these lines to Dockerfile of frontend app to be able to use them to communicate with those k8s services inside cluster:
`
ENV PRODUCTS_SERVICE="products"
ENV SHOPPING_CART_SERVICE="shopping-cart"
`
So these env variables are gonna be endpoint names inside k8s.

service = enables communication

Generally when you have different endpoint configurations and for example you need to connect to a DB or some other service, you usally have this config externally and not
backed into your Dockerfile, so either in docker-compose config or k8s manifest file config, so usually you would set those values from outside, so that you don't have to
go back and change your Dockerfile everytime those values changes.

We're setting those values inside Dockerfile in this case, because we're only have 2 env variables that we're setting and only in frontend we're doing this, but note that
often you would have this configuration set from outside, or you can have combination of both like having them in Dockerfile and then you can **overwrite** them in a k8s deployment
file for example by passing those env variables on the pod config. 


![img.png](../img/section-5/0059_4-10.png)
![img.png](../img/section-5/0059_4-11.png)
### Update "deploy" job:
Now let's write the pipeline logic of deploying the service to k8s cluster. Which in our case means that in the deploy step, we update the logic that now deploys to 
ec2 instance and adjust it to deploy to k8s cluster. In fact, the good thing here is that since we already have this deploy logic extracted in it's own CI templates repo,
we won't have to update this 3 times(for each microservice which in our case we have 3) in each pipeline, instead, we're gonna have to adjust this only once and then just change
the reference to it.

In `ci-templates` repo, rename deploy.yaml to deploy-compose.yaml (compose means docker-compose). So we still keep that logic for deploying to ec2 instance.

Create a new file named `deploy-k8s.yaml` which holds the logic for deploying to k8s there.

![img.png](../img/section-5/0059_4-12.png)
![img.png](../img/section-5/0059_4-13.png)

By setting the KUBECONFIG env variable by saying: `export KUBECONFIG=$KUBE_CONFIG`, `kubectl` command will know which kubeconfig file to use to connect and authenticate with
k8s cluster.

The deploy job in deploy-k8s.yaml wil be executed in pipeline of frontend and other services and it(the job) will be executed from the root of the service project because
that's where we have the .gitlab.ci.yaml file(pipeline config). Now, we're gonna write the script logic of `deploy-k8s.yaml`(which is in a dedicated repo), with the thought of
it's gonna be executed inside the services when we reference it from the root of that service project. So we reference the k8s manifest files with the path of: `kubernetes/deployment.yaml` and
`kubernetes/service.yaml`. So even though we don't have those two files inside ci-templates project, we specify those file locations relative to inside the services from where
we will execute that ci-templates job.

Currently we have a problem: Even though we're exporting(setting) the env variables needed in k8s manifest files, this won't be enough to substitute them as values inside the
deployment and service files. So for example, the $MICRO_SERVICE variable in `service.yaml` and deployment.yaml` , won't get substituted with the value that we set in `deploy-k8s.yaml` .
We need to use a specific tool in order to make this happen and it's called `envsubst` or environment substitution. We use this command to explicitly replace
any values that are parametrized inside those manifest files with the matching env variable values. 

envsubst is a linux program.

![img.png](../img/section-5/0059_4-14.png)

Since we're deploying images from a private docker registry which in our case is on gitlab, this means k8s would need access to it. Note that we don't want
any cluster from anywhere have access to our gitlab docker container registry, we need to give that access explicitly. So we need an equivalent of `docker login` inside k8s,
so that when we create deployment file that references that image which is in the private registry, k8s(or docker on the k8s) can fetch that image from our docker registry.
For this, in k8s we have a especial type of secret called docker registry that can hold credentials to fetch from a private container registry.

So we will create that secret in the cluster using the gitlab credentials.

![img.png](../img/section-5/0059_4-15.png)

So before creating the deployment and service, use `kubectl create secret` command that will create the docker-registry secret. 
Note: `docker-registry` is type of the secret and not a arbitrary name and it's especially for docker registry secrets.

To create a k8s secret:
```yaml
kubectl create secret <secret-type> <secret-name>
```

The registry location itself is configured with: `--docker-server`.

**Important:** We can't use $CI_REGISTRY_USER and $CI_REGISTRY_PASSWORD because the user and password variables for the registry that we have available inside the pipeline, are only valid
as long as the job is running. So once the job is completed, those values are not usable anymore to access the container registry. So we can't use those values in k8s, because
once the job is done, k8s should be able to fetch those images. So we need sth more persistent and we have 2 options here:
If you go to any project that doesn't have images in the container registry, in that empty container registry page, we have some info about how to access
and authenticate with the registry:
1) One option is to use your gitlab username and password in order to access the registry and authenticate with them
2) or if you don't want to use your gitlab account credentials, or if you have a 2-factor authentication enabled, you can create a personal access token and use that instead as password.
So that this would be sth really secure than using username and password. Buf ro simplicity, we're gonna use the gitlab username and password, so that k8s can access the container registry and
know that because you're using the gitlab account credentials, k8s gains access to the projects and ... and not just the container registry.

Now to use username and password of our gitlab account, since they're credentials, we're gonna create them as secret variables. So go to settings>ci/cd create 2 more variables named
GITLAB_USER(you can mask the password, because it doesn't have newlines or any special characters(@ and ... are not considered special in this case) like our secret file variables had, so we can 
use mask)) and GITLAB_PASSWORD(mask this as well).

![img.png](../img/section-5/0059_4-16.png)
![img.png](../img/section-5/0059_4-17.png)

Since we're using `docker create` command instead of `apply`, the issue with `create` command is that if we try to create a resource that already exists with the same name,
we'll get an error and that's the disadvantage of using imperative commands like `create`, `update` and `delete`, compared to declarative commands such as `apply`.
So in order to fix this issue to not get an error after the first run when we created the secret already(remember, the pipeline would run many times!), we're gonna change this
command into `apply`. To do this, we'r'e gonna tell kubectl to create a yaml file from that create command and then apply that yaml file using `kubectl apply`, so we're gonna do:
1) generate secret yaml file from the create command
2) apply yaml file

For this, we have an option on all kubectl commands that's called `--dry-run=client` and it's equivalent of getting a preview of the command without executing it into a k8s yaml file.

Now that we have that yaml file, we can use it in a kubectl apply command.

Now we can create the secret with kubectl apply command, so that next time when the secret already exists and that job runs, kubectl apply will take care of
validating and checking that the secret is already there and will not try to re-create it. Also don't forget to add namespace option(-n) to the kubectl create command, because we need to
create the secret inside the my-micro-service namespace, so that it will be available for all the containers and pods in that namespace, because you can't access
secrets from other namespaces in k8s.

![img.png](../img/section-5/0059_4-18.png)
![img.png](../img/section-5/0059_4-19.png)

In order to use the secret we're creating in the deploy job, in deployment.yaml we need to change sth, to tell k8s from where to pull that image that is specified in the deployment file.
Add `imagePullSecrets` attr inside the specification of the pod:
```yaml
      imagePullSecrets:
      - name: my-registry-key
 
```

You can have multiple apps inside your k8s cluster that are coming from different container registries and for each one you can specify own registry pull secrets.

We can also parametrize imagePullSecrets>name but we have the **same** value for all the services, so we don't need to do that.

![img.png](../img/section-5/0059_4-20.png)

## 0060_5_Deploy_to_Kubernetes_Part_2
### Install kubectl on runner:
Before we execute the pipelines, we need to install kubectl on the gitlab runner which will execute these jobs.

To do this, SSH into the gitlab runner(remote machine that we registered as runner):
```shell
ssh -i <path to .pem file> <user like ubunut>@13.38.115.138
kubectl # running this without installed kubectl will give you a command to install it: sudo snap install kubectl
sudo snap install kubectl # this will give us warnings, so we use an alternative approach to install it by searching install kubectl on ubuntu

sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /usr/share/keyrins/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.png
# You can get the rest of the commands on k8s official documentation
```

Now if you run `kubectl`, we have the command available. We should also check that we have the envsubst command available on the runner(if it's installed, it's gonna wait for input).

### Adjust frontend pipeline configuration & run pipeline:
Include deploy-k8s.yml in include>project>file list. This will make the `deploy` job(the job in that included yaml file) available and make sure the deploy job is written as a 
job in that pipeline. Now we need to provide all the needed env variables.

![img.png](../img/section-5/0060_5-21.png)

If you run the pipeline, you will get an error. Why? Read the remaining... .

### Add secret permission:
The reason of pipeline failing is because the service account that we associated with the gitlab user, is not allowed to create secret in my-micro-service namespace, which is logical
because we didn't allow that specific permission in the role, when we created that. We forgot to put secret resource also on the list of permissions for ci-cd user

![img.png](../img/section-5/0060_5-22.png)
![img.png](../img/section-5/0060_5-23.png)

To fix that, go to the local k8s admin user(to be able to adjust the permissions), by setting the KUBECONFIG env variable to point to the kubeconfig file of that user:
```shell
export KUBECONFIG=<path to related kubeconfig file>
vim cicd-role.yaml
```
Add secrets to the resources of apiGroups in cicd-role.yaml .

```shell
kubectl apply -f cicd-role.yaml
```

now kus knows that whenever a service-account tries to connect to it that has this role that we changed it's file just now, it will allow it to create, update and ... secrets in the
my-micro-service namespace.

### Run pipeline & access frontend:
After running the pipeline(which would run the deploy job to k8s), the frontend app should be running in the cluster, to check the frontend microservice's deployment and service resources have
been created:
```shell
kubectl get service -n my-micro-service
```

![img.png](../img/section-5/0060_5-24.png)
We have 2 pods out of 2 desired pods, running. To check the pods:
```shell
kubectl get pods -n my-micro-service
```
![img.png](../img/section-5/0060_5-24.png)
![img.png](../img/section-5/0060_5-25.png)

To access our frontend app to make sure it's also running properly and it's accessible from browser, since we're using internal services, which means the services are
only accessible from **inside** the cluster, we can use kubectl proxy to access our frontend service, locally from the browser. 

![img.png](../img/section-5/0060_5-26.png)
Kubectl proxy creates a proxy to k8s cluster and makes any service available locally, so we can access it easily without having to configure ingress or external service or ... from our apps.
Note that this is for test purposes only, so that you can test it locally.
![img.png](../img/section-5/0060_5-27.png)

In real cluster, we would configure an ingress controller or load balancer or some kind of external service and ... to allow users to access our apps(we don't learn these stuff
in this course).

To create kubectl proxy:
```shell
kubectl port-forward service/frontend -n my-micro-service 3000:3000
```
Now our frontend service is forwarded locally on localhost:3000 (127.0.0.1), so if we type it in the browser, we can see the app that is running in the cluster.

![img.png](../img/section-5/0060_5-28.png)
![img.png](../img/section-5/0060_5-29.png)

### Configure deployment of other microservices:
It's a best practice in k8s to run at least 2 replicas for each service, so you're not relying on just 1 instance of your service running.
![img.png](../img/section-5/0060_5-30.png)

To test the product service that is running, open a new terminal window and since it's a new terminal session, we have to set the KUBECONFIG env variable there as well:
```shell
export KUBECONFIG=/.../my-micro-service.kubeconfig.yaml # admin user's kubeconfig
kubectl get service -n my-micro-service
kubectl get deployments -n my-micro-service
kubectl get pod -n my-micro-service
```

![img.png](../img/section-5/0060_5-31.png)
![img.png](../img/section-5/0060_5-32.png)

If we break the app which is a k8s service, k8s automatically restarted the pod and now it's up and running again and you can see one restart in the pic below:

![img.png](../img/section-5/0060_5-33.png)

Currently, we have 2 instances of each service which we configured by: `replicas: 2` and we don't have to manage any kind of load balancing(really?!!)(which frontend pod gets the req from the
user's browser and which product pods is gonna communicate to), all of that is handled by k8s.

If we have done the same with docker-compose deployment, when we req to a backend service that doesn't exist(isn't deployed or does't exist generally), the app would
crash and you would have to manually restart the frontend service. So that's one of the advantages of container orchestration tools.

## 0061_6_Wrap_Up_Delete_all_cloud_resources
![img.png](../img/section-5/0061_6-34.png)
![img.png](../img/section-5/0061_6-35.png)

### Delete all resources on cloud platforms:
Delete these:
A gitlab runner that is running on an ec2 instance and a linode k8s engine, once you're done learning, so you don't get charged by cloud providers anymore.
![img.png](../img/section-5/0061_6-36.png)
### WRAP UP AND CLEANUP:
## 0062_Wrap_Up_Congratulations
We learned deploying to docker-compose and k8s envs.